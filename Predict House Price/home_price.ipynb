{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp=pd.read_csv('home_price.csv')\n",
    "hp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "lm1 = smf.ols(formula='price ~  bedrooms + bathrooms + sqft_living+ sqft_lot+ floors +waterfront+view +condition+ grade+sqft_above+sqft_basement+yr_built+ yr_renovated+ zipcode+lat+long+sqft_living15+ sqft_lot15', data=hp).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run a significant test to select features\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# casue p-value for floors is larger than 0.05, delete the floors\n",
    "feature_house1 = ['bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "       'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "X1_house = hp[feature_house1]\n",
    "y_house = hp['price']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_house, y_house,random_state = 42)\n",
    "linreg = LinearRegression().fit(X1_train, y1_train)\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X1_train, y1_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X1_test, y1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that return different combinations of 'bedrooms' and other features(except 'id', 'date', 'price' and 'bedrooms') \n",
    "y=hp['price']\n",
    "iv=hp[hp.columns[3:]]\n",
    "\n",
    "def iv(x):\n",
    "    if x in range(18):\n",
    "        return hp[hp.columns[3:21-x]]\n",
    "    else:\n",
    "        print('wrong number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let price be y and the combinations from In[3] be X, show the coefficients and intercept for different linear models\n",
    "##since deleting floor does not improve the model, we keep it anyway\n",
    "for i in range(18):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iv(i), y,random_state=42)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    print('linear model coeff (w): {}'.format(linreg.coef_))\n",
    "    print('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\n",
    "    print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "    print('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that return ssr for different model using different input features\n",
    "def ssr(i):\n",
    "    if i in range(18):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(iv(i),y,random_state=42)\n",
    "        lr=LinearRegression().fit(X_train, y_train)\n",
    "        y_predict=lr.predict(X_test)\n",
    "        ssr=sum((y_test - y_predict)**2)\n",
    "        return ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iv(17),y,random_state=42)\n",
    "lr=LinearRegression().fit(X_train, y_train)\n",
    "yp=lr.predict(X_test)\n",
    "\n",
    "\n",
    "plt.plot(iv(17),lr.coef_ *iv(17)+lr.intercept_, 'r-')\n",
    "plt.title('linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print the ssr for all different input features\n",
    "#it seems in this case that the input with all variables except 'id', 'date', 'price' has the best ssr result\n",
    "for i in range(18):\n",
    "    print('ssr=',ssr(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize polynomial regressions for each and every input feature(one feature each time) with degree = 3\n",
    "for i in range(3,18):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(hp[hp.columns[i]],y,random_state=42)\n",
    "    coeffs=np.polyfit(X_train,y_train,3)\n",
    "    x2=np.arange(min(hp[hp.columns[i]])-1, max(hp[hp.columns[i]])+1, .01) #use more points for a smoother plot\n",
    "    y2=np.polyval(coeffs, x2) #Evaluates the polynomial for each x2 value\n",
    "    plt.plot(x2,y2)\n",
    "    plt.scatter(hp[hp.columns[i]], y,marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#function that combines poly and lr\n",
    "def PolynomialRegression(degree=i, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let the parameter be degree in range of (1,5)\n",
    "#let X be different input features(one feature each time)\n",
    "#function that returns the train scores and test scores of 3 polynomial regression models using different subset of the data \n",
    "i_range=range(1,5)\n",
    "def fpoly(cl):\n",
    "    if cl in range(3,18):\n",
    "        X=hp[hp.columns[cl]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.values.reshape(-1,1),y,random_state=42)\n",
    "        for i in i_range:\n",
    "            train_scores, test_scores = validation_curve(PolynomialRegression(degree=i).fit(X_train,y_train),X.values.reshape(-1,1),y,'polynomialfeatures__degree',i_range)\n",
    "            return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print train score and test score for different input features and degree = 1,2,3,4\n",
    "#it seems that result for input feature 'sqft_living' with degree=3 is the best\n",
    "for i in range(3,18):\n",
    "    print(fpoly(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_range=range(1,5)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(hp[hp.columns[5]].values.reshape(-1,1),y,random_state=42)\n",
    "train_scores5, test_scores5 = validation_curve(PolynomialRegression(degree=i).fit(X_train5,y_train5),hp[hp.columns[5]].values.reshape(-1,1),y,'polynomialfeatures__degree',i_range)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_scores5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use input feature = 'sqft_living' \n",
    "#plot the mean train and test scores across the 3 models using different subsets of the data for different degre\n",
    "#it seems that degree=3 has the best result\n",
    "\n",
    "plt.figure()\n",
    "train_scores_mean = np.mean(train_scores5, axis=1)\n",
    "test_scores_mean = np.mean(test_scores5, axis=1)\n",
    "plt.xlabel('$\\degree$ (degree)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.4,0.65)\n",
    "\n",
    "lw=3\n",
    "plt.semilogx(i_range, train_scores_mean, label='Train score',\n",
    "            color='green',lw=lw)\n",
    "plt.semilogx(i_range, test_scores_mean, label='Test score',\n",
    "            color='red',lw=lw)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assess the final fit using test data\n",
    "Final_P=PolynomialRegression(degree=3)\n",
    "Final_P.fit(X_train5,y_train5)\n",
    "predicted_price=Final_P.predict(X_test5)\n",
    "trains=Final_P.score(X_train5,y_train5)\n",
    "tests=Final_P.score(X_test5,y_test5)\n",
    "trains,tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use a pre-built implementation of regression to run polynomial regression and visualize polynomial regressions\n",
    "X_train, X_test, y_train, y_test = train_test_split(hp[hp.columns[5]],y,random_state=42)\n",
    "coeffs=np.polyfit(X_train,y_train,3)\n",
    "x3=np.arange(min(hp[hp.columns[5]])-1, max(hp[hp.columns[5]])+1, .01) \n",
    "y3=np.polyval(coeffs, x3) \n",
    "plt.plot(x3,y3)\n",
    "plt.scatter(hp[hp.columns[5]], y,marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale the data so that the result for L1 and L2 will be better\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X5_poly = poly.fit_transform(hp[hp.columns[5]].values.reshape(-1,1))\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5_poly, y,random_state=42)\n",
    "X5_train_scaled=scaler.fit_transform(X5_train)\n",
    "X5_test_scaled=scaler.transform(X5_test)\n",
    "X5_scaled=scaler.transform(X5_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply L2 to the model, it seems that alpha should be 10 or 20\n",
    "for i in [0,1,10,20,50,100,1000]:\n",
    "    linridge=Ridge(alpha=i)\n",
    "    linridge.fit(X5_train_scaled, y5_train)\n",
    "    train1=linridge.score(X5_train_scaled, y5_train)\n",
    "    test1=linridge.score(X5_test_scaled, y5_test)\n",
    "    print(train1,test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use validation to prove that the best penalty for L2 should be alpha=10\n",
    "l=[0,1,10,20,50,100,500,1000]\n",
    "for j in l:\n",
    "    train_scoresL2, test_scoresL2 = validation_curve(Ridge(j).fit(X5_train_scaled,y5_train),X5_scaled,y,'alpha',l,cv=10)\n",
    "trainL2=train_scoresL2.sum(axis=1)/10\n",
    "testL2=test_scoresL2.sum(axis=1)/10\n",
    "trainL2,testL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualization of polynomial regressions under L2 regularization\n",
    "r=Ridge(alpha=10)\n",
    "r.fit(X5_train_scaled,y5_train)\n",
    "poly=PolynomialFeatures(degree=3)\n",
    "x3p=poly.fit_transform(x3.reshape(-1,1))\n",
    "x3s=scaler.transform(x3p)\n",
    "yp=r.predict(x3s)\n",
    "\n",
    "plt.scatter(hp[hp.columns[5]], y,marker='o',label='actual price')\n",
    "plt.plot(x3,y3,label='poly',color='orange')\n",
    "plt.plot(x3,yp,label='poly under L2',color='green')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assess the final fit using test data\n",
    "Final_L2=Ridge(alpha=10)\n",
    "Final_L2.fit(X5_train_scaled,y5_train)\n",
    "predicted_priceL2=Final_L2.predict(X5_test_scaled)\n",
    "trainF2=Final_L2.score(X5_train_scaled,y_train5)\n",
    "testF2=Final_L2.score(X5_test_scaled,y_test5)\n",
    "trainF2,testF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply L1 to the model, split the data to training and testing, it seems that alpha should be 1000\n",
    "for i in [1,10,20,50,100,1000,1100]:\n",
    "    linlasso=Lasso(alpha=i, max_iter = 10000).fit(X5_train_scaled, y5_train)\n",
    "    train2=linlasso.score(X5_train_scaled, y5_train)\n",
    "    test2=linlasso.score(X5_test_scaled, y5_test)\n",
    "    print(train2,test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use validation to show the best penalty, it seems that L1's alpha should be 200\n",
    "#we will use alpha=200 instead\n",
    "\n",
    "l=[10,50,100,200,300,400,500,1000]\n",
    "for j in l:\n",
    "    train_scoresL1, test_scoresL1 = validation_curve(Lasso(alpha=j),X5_scaled,y,'alpha',l,cv=10)\n",
    "trainL1=train_scoresL1.sum(axis=1)/10\n",
    "testL1=test_scoresL1.sum(axis=1)/10\n",
    "trainL1,testL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assess the final fit using test data\n",
    "Final_L1=Lasso(alpha=200)\n",
    "Final_L1.fit(X5_train_scaled,y5_train)\n",
    "predicted_priceL1=Final_L1.predict(X5_test_scaled)\n",
    "trainF1=Final_L1.score(X5_train_scaled,y_train5)\n",
    "testF1=Final_L1.score(X5_test_scaled,y_test5)\n",
    "trainF1,testF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from the coefficients we can see that the list of significant features should be sqft_living^1 and sqft_living^2\n",
    "coefs = []\n",
    "linlasso=Lasso(alpha=200)\n",
    "linlasso.fit(X5_train_scaled, y5_train)\n",
    "coefs=linlasso.coef_\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose all features as input in the knn model, use validation curve to find the best n\n",
    "##this step may take a long time to run, please be patient\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "k_range=[1,3,5,10,15,20,25,30]\n",
    "X_train, X_test, y_train, y_test = train_test_split(iv(1), y, random_state = 42)\n",
    "for k in k_range:\n",
    "    train_scores, test_scores = validation_curve(KNeighborsRegressor(n_neighbors=k), iv(1), y, 'n_neighbors', k_range) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean=np.mean(train_scores, axis=1)\n",
    "test_scores_mean=np.mean(test_scores, axis=1)\n",
    "train_scores_mean,test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yellow dots:training scores, blue dots:test scores\n",
    "#plot the mean score of training and testing for the model(input features:all except id, date and price)\n",
    "#we pick k=15 as the best n\n",
    "for k in k_range:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n",
    "    \n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, train_scores_mean, color='orange')\n",
    "plt.scatter(k_range, test_scores_mean, color='navy')\n",
    "plt.xticks([0,5,10,15,20,25,30,35]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use sqft_living as input feature, use validation curve to find the best n\n",
    "X5= hp[hp.columns[5]].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X5, y, random_state = 42)\n",
    "train_scores, test_scores = validation_curve(KNeighborsRegressor(n_neighbors = k), X5, y, param_name='n_neighbors', param_range= k_range) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "train_scores_mean,test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best n=30\n",
    "for k in k_range:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n",
    "    \n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, train_scores_mean, color='orange')\n",
    "plt.scatter(k_range, test_scores_mean, color='navy')\n",
    "plt.xticks([0,5,10,15,20,25,30,35]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input feature: grade, use validation curve to find the best n\n",
    "X11= hp[hp.columns[11]].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X11, y, random_state = 42)\n",
    "train_scores, test_scores = validation_curve(KNeighborsRegressor(n_neighbors = k), X11, y, param_name='n_neighbors', param_range= k_range) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "train_scores_mean,test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best n=25\n",
    "for k in k_range:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n",
    "    \n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, train_scores_mean, color='orange')\n",
    "plt.scatter(k_range, test_scores_mean, color='navy')\n",
    "plt.xticks([0,5,10,15,20,25,30,35]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input feature: grade and sqft_living, use validation curve to find the best n\n",
    "Xsg= hp[['sqft_living','grade']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xsg, y, random_state = 42)\n",
    "train_scores, test_scores = validation_curve(KNeighborsRegressor(n_neighbors = k), Xsg, y, param_name='n_neighbors', param_range= k_range) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "train_scores_mean,test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best n=20\n",
    "for k in k_range:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = k).fit(X_train, y_train)\n",
    "    \n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, train_scores_mean, color='orange')\n",
    "plt.scatter(k_range, test_scores_mean, color='navy')\n",
    "plt.xticks([0,5,10,15,20,25,30,35]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from all these model we can see that the best one is n=15 and input features are all except id, date and price\n",
    "#predict price using the k-nearest neighbors\n",
    "X_train, X_test, y_train, y_test = train_test_split(iv(1), y, random_state = 42)\n",
    "KR=KNeighborsRegressor(n_neighbors=15).fit(X_train,y_train)\n",
    "predictP=KR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
